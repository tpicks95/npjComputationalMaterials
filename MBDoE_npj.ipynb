{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fd92bd-e153-4ab9-b082-bbfb47646091",
   "metadata": {},
   "source": [
    "# Benchmarking model-based design of experiment approaches with a pharmaceutical crystallisation emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbdab5-e7b5-4f3f-804a-dc8b529a4f12",
   "metadata": {},
   "source": [
    "**Installing the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5651b0-ba2e-4ed6-b211-57d6d7bb1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obsidian import Campaign, ParamSpace, Target\n",
    "from obsidian.parameters import Param_Categorical, Param_Ordinal, Param_Continuous, Param_Discrete\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from pymoo.indicators.hv import HV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import load\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec4fe4-3243-41b6-b26c-ee76cc8675e7",
   "metadata": {},
   "source": [
    "**Define parameters and targets for optimisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3675f6-07f5-4527-8c4e-5518732b1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    Param_Continuous('Initial_temperature', 34, 40),\n",
    "    Param_Continuous('Seed_load', 0, 2.625),\n",
    "    Param_Ordinal('Seed_mean', ['46.94', '55.94']),\n",
    "    Param_Continuous('Final_temperature', 10, 25),\n",
    "    Param_Continuous('Cooling_rate', 0.1, 0.5),\n",
    "    Param_Continuous('AS_end_frac', 0.45, 0.9),\n",
    "    Param_Continuous('AS_rate_mL_min', 3, 10)\n",
    "    ]\n",
    "\n",
    "param_column_names = [p.name for p in params]\n",
    "\n",
    "X_space = ParamSpace(params)\n",
    "\n",
    "target = [\n",
    "    Target('d90', aim='min'),\n",
    "    Target('Yield', aim='max'),\n",
    "]\n",
    "\n",
    "target_column_names = [t.name for t in target]\n",
    "\n",
    "campaign = Campaign(X_space, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82eecd-33b8-4a15-9a9f-31ff3e1d8a97",
   "metadata": {},
   "source": [
    "**Loading the simulated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a53c2-ffc3-494b-84ee-862e58f21b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read the dataset from the Excel file\n",
    "    data = pd.read_excel(r\"file_location.xlsx\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fefe3-228d-49ff-a0bd-fdc25b26f52f",
   "metadata": {},
   "source": [
    "**Random forest model**\n",
    "\n",
    "This is specific to the optimisation problem and therefore needs to be run whenever inputs and outputs are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bee014-5ecd-41c0-9572-fc5b81db2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    input_features = param_column_names\n",
    "    output_features = target_column_names\n",
    "\n",
    "    X = data[input_features].values\n",
    "    y = data[output_features].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def build_model():\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    data = load_data()\n",
    "    X_train, X_test, y_train, y_test, scaler = preprocess_data(data)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Test MSE: {mse}, Test MAE: {mae}\")\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler = main()\n",
    "    dump(model, 'model.joblib')\n",
    "    dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c24859-893a-4b62-ae80-f1d4eb465553",
   "metadata": {},
   "source": [
    "**Model-based design of experiment - Bayesian optimisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff043c3-f4c9-4db6-94b1-35e19d9adb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0_list = []\n",
    "\n",
    "num_exp_list = [5, 10, 15, 20]\n",
    "design_methods = ['LHS', 'Random', 'Sobol']\n",
    "acquisition_functions = ['Mean', 'RS', 'SF', 'EHVI', 'NEHVI', 'NParEGO']\n",
    "\n",
    "model = load('model.joblib')\n",
    "scaler = load('scaler.joblib')\n",
    "\n",
    "\n",
    "for num_exp in num_exp_list:\n",
    "    for design_method in design_methods:\n",
    "        # Initial Screening\n",
    "        X0 = campaign.designer.initialize(num_exp, design_method)\n",
    "\n",
    "        new_X = X0[param_column_names].values\n",
    "        new_X_scaled = scaler.transform(new_X)\n",
    "\n",
    "        predictions = model.predict(new_X_scaled)\n",
    "        predictions_df = pd.DataFrame(predictions, columns=target_column_names, index=X0.index)\n",
    "\n",
    "        Z0_initial = pd.concat([X0, predictions_df], axis=1)\n",
    "        Z0_initial['Method'] = design_method\n",
    "\n",
    "        # BO initialisation\n",
    "        X0 = Z0_initial[param_column_names]\n",
    "        Y0 = Z0_initial[target_column_names]\n",
    "\n",
    "        normalized_Y0 = (Y0 - Y0.min()) / (Y0.max() - Y0.min())\n",
    "        normalized_Z0 = pd.concat([X0, normalized_Y0], axis=1)\n",
    "        \n",
    "        campaign.add_data(normalized_Z0)\n",
    "        campaign.fit()\n",
    "\n",
    "        for acquisition_function in acquisition_functions:\n",
    "            Z0 = Z0_initial.copy()\n",
    "\n",
    "            for _ in range(5):\n",
    "                X_suggest, eval_suggest = campaign.suggest(acquisition=[acquisition_function])\n",
    "\n",
    "                new_X = X_suggest[param_column_names].values\n",
    "                new_X_scaled = scaler.transform(new_X)\n",
    "                predictions = model.predict(new_X_scaled)\n",
    "                predictions_df = pd.DataFrame(predictions, columns=target_column_names, index=X_suggest.index)\n",
    "\n",
    "                new_rows = pd.concat([X_suggest, predictions_df], axis=1)\n",
    "                new_rows['Method'] = acquisition_function\n",
    "\n",
    "                Z0 = pd.concat([Z0, new_rows], axis=0)\n",
    "\n",
    "                X_new = new_rows[param_column_names]\n",
    "                Y_new = new_rows[target_column_names]\n",
    "                normalized_Y_new = (Y_new - Y0.min()) / (Y0.max() - Y0.min())  # normalize using original Y0 stats\n",
    "                normalized_Z_new = pd.concat([X_new, normalized_Y_new], axis=1)\n",
    "\n",
    "                campaign.add_data(normalized_Z_new)\n",
    "                campaign.fit()\n",
    "\n",
    "            Z0_list.append(Z0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b0fb9-9aa9-480c-bfda-66c880b9a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0_list[71]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1750e-1952-4710-9320-5a8565770f65",
   "metadata": {},
   "source": [
    "**Hypervolume metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623b754-b607-4566-989d-82f163502b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Z0 in Z0_list:\n",
    "\n",
    "    Y0 = Z0[target_column_names]\n",
    "\n",
    "    #This is manual upper and lower bounds for d90 and yield\n",
    "    new_rows = pd.DataFrame([\n",
    "        [78, 17],\n",
    "        [174, 99]\n",
    "    ], columns=target_column_names)\n",
    "\n",
    "    Y0 = pd.concat([Y0, new_rows], ignore_index=True)\n",
    "    \n",
    "    normalized_Y0 = (Y0 - Y0.min()) / (Y0.max() - Y0.min())\n",
    "\n",
    "    normalized_Y0 = normalized_Y0.iloc[:-2]\n",
    "    \n",
    "    for t in target:\n",
    "        if t.aim == 'max':\n",
    "            normalized_Y0[t.name] = -normalized_Y0[t.name]  # Invert so we minimize all\n",
    "    \n",
    "    solutions = normalized_Y0.to_numpy()\n",
    "    \n",
    "    reference_point = [1.1 if t.aim == 'min' else -0.1 for t in target]\n",
    "    \n",
    "    hv = HV(ref_point=reference_point)\n",
    "    \n",
    "    hypervolume_values = []\n",
    "    for i in range(1, len(solutions) + 1):\n",
    "        subset = solutions[:i]\n",
    "        hypervolume_values.append(hv.do(subset))\n",
    "    \n",
    "    Z0['Hypervolume'] = hypervolume_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea11794-ce03-42fd-ab0c-5351e7059bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0_list[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecaeaf-4f0c-4e68-8768-501b2ec784f8",
   "metadata": {},
   "source": [
    "**Saving the data as csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c60a22-5b24-42b8-90e3-fd27a0af4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(Z0_list):\n",
    "    first_method = df['Method'].iloc[0]\n",
    "    last_method = df['Method'].iloc[-1]\n",
    "    exp = len(df) - 5\n",
    "\n",
    "    filename = f\"df_{exp}_{first_method}_{last_method}_campaign5.csv\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab6145-d02e-4c13-9a40-d99774b54615",
   "metadata": {},
   "source": [
    "**Figures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87eb72-1c4a-4f07-af0d-c0d793adbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Z0_list[71]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fe05d-93e3-457c-a589-16812d9735f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, len(df) + 1)\n",
    "y = df[[\"Hypervolume\"]].to_numpy().ravel()\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Hypervolume\", fontsize=12)\n",
    "plt.xlabel(\"Number of experiments\", fontsize=12)\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7e4d4-f706-427a-87a6-4d7137fcbc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
