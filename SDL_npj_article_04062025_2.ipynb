{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fd92bd-e153-4ab9-b082-bbfb47646091",
   "metadata": {},
   "source": [
    "# Benchmarking model-based design of experiment approaches with a pharmaceutical crystallisation emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbdab5-e7b5-4f3f-804a-dc8b529a4f12",
   "metadata": {},
   "source": [
    "**Installing the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5651b0-ba2e-4ed6-b211-57d6d7bb1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obsidian import Campaign, ParamSpace, Target\n",
    "from obsidian.parameters import Param_Categorical, Param_Ordinal, Param_Continuous, Param_Discrete\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from pymoo.indicators.hv import HV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import load\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec4fe4-3243-41b6-b26c-ee76cc8675e7",
   "metadata": {},
   "source": [
    "**Define parameters and targets for optimisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3675f6-07f5-4527-8c4e-5518732b1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    Param_Continuous('Initial_temperature', 34, 40),\n",
    "    Param_Continuous('Seed_load', 0, 2.625),\n",
    "    Param_Ordinal('Seed_mean', ['46.94', '55.94']),\n",
    "    Param_Continuous('Final_temperature', 10, 25),\n",
    "    Param_Continuous('Cooling_rate', 0.1, 0.5),\n",
    "    Param_Continuous('AS_end_frac', 0.45, 0.9),\n",
    "    Param_Continuous('AS_rate_mL_min', 3, 10)\n",
    "    ]\n",
    "\n",
    "param_column_names = [p.name for p in params]\n",
    "\n",
    "X_space = ParamSpace(params)\n",
    "\n",
    "target = [\n",
    "    Target('d90', aim='min'),\n",
    "    Target('Yield', aim='max'),\n",
    "]\n",
    "\n",
    "target_column_names = [t.name for t in target]\n",
    "\n",
    "campaign = Campaign(X_space, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82eecd-33b8-4a15-9a9f-31ff3e1d8a97",
   "metadata": {},
   "source": [
    "**Loading the simulated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0a53c2-ffc3-494b-84ee-862e58f21b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read the dataset from the Excel file\n",
    "    data = pd.read_excel(r\"I:\\Science\\SIPBS\\cmac\\Thomas Pickles\\SDL\\lovastatin_gFP_GSA.xlsx\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fefe3-228d-49ff-a0bd-fdc25b26f52f",
   "metadata": {},
   "source": [
    "**Random forest model**\n",
    "\n",
    "This is specific to the optimisation problem and therefore needs to be run whenever inputs and outputs are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bee014-5ecd-41c0-9572-fc5b81db2778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 2.70736632921691, Test MAE: 1.047529776622861\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    input_features = param_column_names\n",
    "    output_features = target_column_names\n",
    "\n",
    "    X = data[input_features].values\n",
    "    y = data[output_features].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def build_model():\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    data = load_data()\n",
    "    X_train, X_test, y_train, y_test, scaler = preprocess_data(data)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Test MSE: {mse}, Test MAE: {mae}\")\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler = main()\n",
    "    dump(model, 'model.joblib')\n",
    "    dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c24859-893a-4b62-ae80-f1d4eb465553",
   "metadata": {},
   "source": [
    "**Model-based design of experiment - Bayesian optimisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff043c3-f4c9-4db6-94b1-35e19d9adb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.995 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.99 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.987 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.985 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.979 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.979 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.979 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.979 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.98 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.98 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.984 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.984 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.984 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.982 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.982 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.982 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.983 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.981 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.982 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.981 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.98 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.98 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\botorch\\optim\\optimize.py:564: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  return _optimize_acqf_batch(opt_inputs=opt_inputs)\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\botorch\\optim\\optimize.py:564: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
      "  return _optimize_acqf_batch(opt_inputs=opt_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.978 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.977 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.976 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.975 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.974 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.973 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.971 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.97 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.97 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.969 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.969 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.969 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.969 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.968 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.965 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 0.999 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\pxb20160\\AppData\\Local\\anaconda3v2\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.967 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.965 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n",
      "GP model has been fit to data with an R2-train-score of: 0.966 for response: d90\n",
      "GP model has been fit to data with an R2-train-score of: 1 for response: Yield\n"
     ]
    }
   ],
   "source": [
    "Z0_list = []\n",
    "\n",
    "num_exp_list = [5, 10, 15, 20]\n",
    "design_methods = ['LHS', 'Random', 'Sobol']\n",
    "acquisition_functions = ['Mean', 'RS', 'SF', 'EHVI', 'NEHVI', 'NParEGO']\n",
    "\n",
    "model = load('model.joblib')\n",
    "scaler = load('scaler.joblib')\n",
    "\n",
    "\n",
    "for num_exp in num_exp_list:\n",
    "    for design_method in design_methods:\n",
    "        # Initial Screening\n",
    "        X0 = campaign.designer.initialize(num_exp, design_method)\n",
    "\n",
    "        new_X = X0[param_column_names].values\n",
    "        new_X_scaled = scaler.transform(new_X)\n",
    "\n",
    "        predictions = model.predict(new_X_scaled)\n",
    "        predictions_df = pd.DataFrame(predictions, columns=target_column_names, index=X0.index)\n",
    "\n",
    "        Z0_initial = pd.concat([X0, predictions_df], axis=1)\n",
    "        Z0_initial['Method'] = design_method\n",
    "\n",
    "        # BO initialisation\n",
    "        X0 = Z0_initial[param_column_names]\n",
    "        Y0 = Z0_initial[target_column_names]\n",
    "\n",
    "        normalized_Y0 = (Y0 - Y0.min()) / (Y0.max() - Y0.min())\n",
    "        normalized_Z0 = pd.concat([X0, normalized_Y0], axis=1)\n",
    "        \n",
    "        campaign.add_data(normalized_Z0)\n",
    "        campaign.fit()\n",
    "\n",
    "        for acquisition_function in acquisition_functions:\n",
    "            Z0 = Z0_initial.copy()\n",
    "\n",
    "            for _ in range(5):\n",
    "                X_suggest, eval_suggest = campaign.suggest(acquisition=[acquisition_function])\n",
    "\n",
    "                new_X = X_suggest[param_column_names].values\n",
    "                new_X_scaled = scaler.transform(new_X)\n",
    "                predictions = model.predict(new_X_scaled)\n",
    "                predictions_df = pd.DataFrame(predictions, columns=target_column_names, index=X_suggest.index)\n",
    "\n",
    "                new_rows = pd.concat([X_suggest, predictions_df], axis=1)\n",
    "                new_rows['Method'] = acquisition_function\n",
    "\n",
    "                Z0 = pd.concat([Z0, new_rows], axis=0)\n",
    "\n",
    "                X_new = new_rows[param_column_names]\n",
    "                Y_new = new_rows[target_column_names]\n",
    "                normalized_Y_new = (Y_new - Y0.min()) / (Y0.max() - Y0.min())  # normalize using original Y0 stats\n",
    "                normalized_Z_new = pd.concat([X_new, normalized_Y_new], axis=1)\n",
    "\n",
    "                campaign.add_data(normalized_Z_new)\n",
    "                campaign.fit()\n",
    "\n",
    "            Z0_list.append(Z0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38b0fb9-9aa9-480c-bfda-66c880b9a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial_temperature</th>\n",
       "      <th>Seed_load</th>\n",
       "      <th>Seed_mean</th>\n",
       "      <th>Final_temperature</th>\n",
       "      <th>Cooling_rate</th>\n",
       "      <th>AS_end_frac</th>\n",
       "      <th>AS_rate_mL_min</th>\n",
       "      <th>d90</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.093945</td>\n",
       "      <td>1.353894</td>\n",
       "      <td>55.94</td>\n",
       "      <td>21.418427</td>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.643533</td>\n",
       "      <td>8.411758</td>\n",
       "      <td>146.089986</td>\n",
       "      <td>91.022600</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.286633</td>\n",
       "      <td>0.684544</td>\n",
       "      <td>46.94</td>\n",
       "      <td>11.656175</td>\n",
       "      <td>0.322210</td>\n",
       "      <td>0.710433</td>\n",
       "      <td>5.964086</td>\n",
       "      <td>142.904591</td>\n",
       "      <td>87.735671</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.388577</td>\n",
       "      <td>2.526554</td>\n",
       "      <td>55.94</td>\n",
       "      <td>20.117607</td>\n",
       "      <td>0.434281</td>\n",
       "      <td>0.806839</td>\n",
       "      <td>7.692301</td>\n",
       "      <td>166.974901</td>\n",
       "      <td>88.743942</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.675751</td>\n",
       "      <td>0.499122</td>\n",
       "      <td>46.94</td>\n",
       "      <td>16.690746</td>\n",
       "      <td>0.202976</td>\n",
       "      <td>0.536472</td>\n",
       "      <td>3.057075</td>\n",
       "      <td>139.615032</td>\n",
       "      <td>96.337060</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.672997</td>\n",
       "      <td>2.060981</td>\n",
       "      <td>46.94</td>\n",
       "      <td>15.488294</td>\n",
       "      <td>0.281844</td>\n",
       "      <td>0.605451</td>\n",
       "      <td>6.531296</td>\n",
       "      <td>150.999505</td>\n",
       "      <td>93.209075</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38.889118</td>\n",
       "      <td>0.145706</td>\n",
       "      <td>55.94</td>\n",
       "      <td>17.688276</td>\n",
       "      <td>0.450722</td>\n",
       "      <td>0.735412</td>\n",
       "      <td>4.111902</td>\n",
       "      <td>142.422334</td>\n",
       "      <td>79.601737</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.342651</td>\n",
       "      <td>1.913070</td>\n",
       "      <td>46.94</td>\n",
       "      <td>13.037971</td>\n",
       "      <td>0.393264</td>\n",
       "      <td>0.895538</td>\n",
       "      <td>9.544317</td>\n",
       "      <td>163.151200</td>\n",
       "      <td>92.945435</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.653259</td>\n",
       "      <td>1.275127</td>\n",
       "      <td>55.94</td>\n",
       "      <td>23.902561</td>\n",
       "      <td>0.174459</td>\n",
       "      <td>0.462793</td>\n",
       "      <td>4.937263</td>\n",
       "      <td>146.379306</td>\n",
       "      <td>96.577549</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34.282478</td>\n",
       "      <td>2.336503</td>\n",
       "      <td>46.94</td>\n",
       "      <td>19.141594</td>\n",
       "      <td>0.346240</td>\n",
       "      <td>0.515423</td>\n",
       "      <td>6.949480</td>\n",
       "      <td>147.302522</td>\n",
       "      <td>96.620276</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38.092354</td>\n",
       "      <td>0.380288</td>\n",
       "      <td>55.94</td>\n",
       "      <td>13.901997</td>\n",
       "      <td>0.114850</td>\n",
       "      <td>0.842039</td>\n",
       "      <td>4.585600</td>\n",
       "      <td>145.599356</td>\n",
       "      <td>83.757033</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38.670658</td>\n",
       "      <td>1.493950</td>\n",
       "      <td>46.94</td>\n",
       "      <td>24.207062</td>\n",
       "      <td>0.228556</td>\n",
       "      <td>0.675321</td>\n",
       "      <td>9.945408</td>\n",
       "      <td>159.942464</td>\n",
       "      <td>89.420680</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36.387512</td>\n",
       "      <td>0.896938</td>\n",
       "      <td>55.94</td>\n",
       "      <td>12.632074</td>\n",
       "      <td>0.409873</td>\n",
       "      <td>0.664672</td>\n",
       "      <td>5.393867</td>\n",
       "      <td>148.083820</td>\n",
       "      <td>88.553094</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35.890892</td>\n",
       "      <td>1.796138</td>\n",
       "      <td>55.94</td>\n",
       "      <td>10.201728</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.483849</td>\n",
       "      <td>8.817307</td>\n",
       "      <td>154.840691</td>\n",
       "      <td>97.614090</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>39.677334</td>\n",
       "      <td>1.085697</td>\n",
       "      <td>46.94</td>\n",
       "      <td>23.003567</td>\n",
       "      <td>0.262110</td>\n",
       "      <td>0.860344</td>\n",
       "      <td>6.423494</td>\n",
       "      <td>159.686140</td>\n",
       "      <td>83.291060</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37.654140</td>\n",
       "      <td>2.268909</td>\n",
       "      <td>55.94</td>\n",
       "      <td>16.387402</td>\n",
       "      <td>0.193788</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>8.108108</td>\n",
       "      <td>159.558053</td>\n",
       "      <td>91.286076</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.347565</td>\n",
       "      <td>0.282577</td>\n",
       "      <td>46.94</td>\n",
       "      <td>20.524632</td>\n",
       "      <td>0.362605</td>\n",
       "      <td>0.584319</td>\n",
       "      <td>3.526741</td>\n",
       "      <td>136.552120</td>\n",
       "      <td>93.276928</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35.178776</td>\n",
       "      <td>2.146774</td>\n",
       "      <td>55.94</td>\n",
       "      <td>13.543275</td>\n",
       "      <td>0.223128</td>\n",
       "      <td>0.857164</td>\n",
       "      <td>5.762950</td>\n",
       "      <td>144.558428</td>\n",
       "      <td>86.120168</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37.395264</td>\n",
       "      <td>0.241908</td>\n",
       "      <td>46.94</td>\n",
       "      <td>23.294575</td>\n",
       "      <td>0.441957</td>\n",
       "      <td>0.501969</td>\n",
       "      <td>8.592594</td>\n",
       "      <td>145.028948</td>\n",
       "      <td>96.536278</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.837242</td>\n",
       "      <td>1.673463</td>\n",
       "      <td>55.94</td>\n",
       "      <td>14.807302</td>\n",
       "      <td>0.301736</td>\n",
       "      <td>0.567284</td>\n",
       "      <td>3.313947</td>\n",
       "      <td>167.512251</td>\n",
       "      <td>91.586760</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36.146748</td>\n",
       "      <td>1.045609</td>\n",
       "      <td>46.94</td>\n",
       "      <td>18.237745</td>\n",
       "      <td>0.132882</td>\n",
       "      <td>0.774383</td>\n",
       "      <td>7.456145</td>\n",
       "      <td>147.300731</td>\n",
       "      <td>90.210816</td>\n",
       "      <td>Sobol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.524056</td>\n",
       "      <td>0.121283</td>\n",
       "      <td>46.94</td>\n",
       "      <td>19.391901</td>\n",
       "      <td>0.290838</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.639317</td>\n",
       "      <td>131.384754</td>\n",
       "      <td>82.905651</td>\n",
       "      <td>NParEGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.131449</td>\n",
       "      <td>55.94</td>\n",
       "      <td>20.284864</td>\n",
       "      <td>0.434473</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.964757</td>\n",
       "      <td>131.635798</td>\n",
       "      <td>84.828691</td>\n",
       "      <td>NParEGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.94</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.273331</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>6.382882</td>\n",
       "      <td>131.395426</td>\n",
       "      <td>84.487792</td>\n",
       "      <td>NParEGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.94</td>\n",
       "      <td>21.614560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.828337</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>126.075682</td>\n",
       "      <td>72.477398</td>\n",
       "      <td>NParEGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.445285</td>\n",
       "      <td>55.94</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.736196</td>\n",
       "      <td>128.815573</td>\n",
       "      <td>73.844219</td>\n",
       "      <td>NParEGO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Initial_temperature  Seed_load Seed_mean  Final_temperature  Cooling_rate  \\\n",
       "0             35.093945   1.353894     55.94          21.418427      0.140832   \n",
       "1             37.286633   0.684544     46.94          11.656175      0.322210   \n",
       "2             39.388577   2.526554     55.94          20.117607      0.434281   \n",
       "3             35.675751   0.499122     46.94          16.690746      0.202976   \n",
       "4             36.672997   2.060981     46.94          15.488294      0.281844   \n",
       "5             38.889118   0.145706     55.94          17.688276      0.450722   \n",
       "6             38.342651   1.913070     46.94          13.037971      0.393264   \n",
       "7             34.653259   1.275127     55.94          23.902561      0.174459   \n",
       "8             34.282478   2.336503     46.94          19.141594      0.346240   \n",
       "9             38.092354   0.380288     55.94          13.901997      0.114850   \n",
       "10            38.670658   1.493950     46.94          24.207062      0.228556   \n",
       "11            36.387512   0.896938     55.94          12.632074      0.409873   \n",
       "12            35.890892   1.796138     55.94          10.201728      0.481000   \n",
       "13            39.677334   1.085697     46.94          23.003567      0.262110   \n",
       "14            37.654140   2.268909     55.94          16.387402      0.193788   \n",
       "15            35.347565   0.282577     46.94          20.524632      0.362605   \n",
       "16            35.178776   2.146774     55.94          13.543275      0.223128   \n",
       "17            37.395264   0.241908     46.94          23.294575      0.441957   \n",
       "18            39.837242   1.673463     55.94          14.807302      0.301736   \n",
       "19            36.146748   1.045609     46.94          18.237745      0.132882   \n",
       "0             34.524056   0.121283     46.94          19.391901      0.290838   \n",
       "0             34.000000   0.131449     55.94          20.284864      0.434473   \n",
       "0             34.000000   0.000000     46.94          25.000000      0.273331   \n",
       "0             34.000000   0.000000     55.94          21.614560      0.500000   \n",
       "0             34.000000   0.445285     55.94          25.000000      0.100000   \n",
       "\n",
       "    AS_end_frac  AS_rate_mL_min         d90      Yield   Method  \n",
       "0      0.643533        8.411758  146.089986  91.022600    Sobol  \n",
       "1      0.710433        5.964086  142.904591  87.735671    Sobol  \n",
       "2      0.806839        7.692301  166.974901  88.743942    Sobol  \n",
       "3      0.536472        3.057075  139.615032  96.337060    Sobol  \n",
       "4      0.605451        6.531296  150.999505  93.209075    Sobol  \n",
       "5      0.735412        4.111902  142.422334  79.601737    Sobol  \n",
       "6      0.895538        9.544317  163.151200  92.945435    Sobol  \n",
       "7      0.462793        4.937263  146.379306  96.577549    Sobol  \n",
       "8      0.515423        6.949480  147.302522  96.620276    Sobol  \n",
       "9      0.842039        4.585600  145.599356  83.757033    Sobol  \n",
       "10     0.675321        9.945408  159.942464  89.420680    Sobol  \n",
       "11     0.664672        5.393867  148.083820  88.553094    Sobol  \n",
       "12     0.483849        8.817307  154.840691  97.614090    Sobol  \n",
       "13     0.860344        6.423494  159.686140  83.291060    Sobol  \n",
       "14     0.770530        8.108108  159.558053  91.286076    Sobol  \n",
       "15     0.584319        3.526741  136.552120  93.276928    Sobol  \n",
       "16     0.857164        5.762950  144.558428  86.120168    Sobol  \n",
       "17     0.501969        8.592594  145.028948  96.536278    Sobol  \n",
       "18     0.567284        3.313947  167.512251  91.586760    Sobol  \n",
       "19     0.774383        7.456145  147.300731  90.210816    Sobol  \n",
       "0      0.900000        5.639317  131.384754  82.905651  NParEGO  \n",
       "0      0.900000        5.964757  131.635798  84.828691  NParEGO  \n",
       "0      0.900000        6.382882  131.395426  84.487792  NParEGO  \n",
       "0      0.828337        3.000000  126.075682  72.477398  NParEGO  \n",
       "0      0.900000        3.736196  128.815573  73.844219  NParEGO  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z0_list[71]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1750e-1952-4710-9320-5a8565770f65",
   "metadata": {},
   "source": [
    "**Hypervolume metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8623b754-b607-4566-989d-82f163502b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Z0 in Z0_list:\n",
    "\n",
    "    Y0 = Z0[target_column_names]\n",
    "\n",
    "    #This is manual upper and lower bounds for d90 and yield\n",
    "    new_rows = pd.DataFrame([\n",
    "        [78, 17],\n",
    "        [174, 99]\n",
    "    ], columns=target_column_names)\n",
    "\n",
    "    Y0 = pd.concat([Y0, new_rows], ignore_index=True)\n",
    "    \n",
    "    normalized_Y0 = (Y0 - Y0.min()) / (Y0.max() - Y0.min())\n",
    "\n",
    "    normalized_Y0 = normalized_Y0.iloc[:-2]\n",
    "    \n",
    "    for t in target:\n",
    "        if t.aim == 'max':\n",
    "            normalized_Y0[t.name] = -normalized_Y0[t.name]  # Invert so we minimize all\n",
    "    \n",
    "    solutions = normalized_Y0.to_numpy()\n",
    "    \n",
    "    reference_point = [1.1 if t.aim == 'min' else -0.1 for t in target]\n",
    "    \n",
    "    hv = HV(ref_point=reference_point)\n",
    "    \n",
    "    hypervolume_values = []\n",
    "    for i in range(1, len(solutions) + 1):\n",
    "        subset = solutions[:i]\n",
    "        hypervolume_values.append(hv.do(subset))\n",
    "    \n",
    "    Z0['Hypervolume'] = hypervolume_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea11794-ce03-42fd-ab0c-5351e7059bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial_temperature</th>\n",
       "      <th>Seed_load</th>\n",
       "      <th>Seed_mean</th>\n",
       "      <th>Final_temperature</th>\n",
       "      <th>Cooling_rate</th>\n",
       "      <th>AS_end_frac</th>\n",
       "      <th>AS_rate_mL_min</th>\n",
       "      <th>d90</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Method</th>\n",
       "      <th>Hypervolume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.800000</td>\n",
       "      <td>1.837500</td>\n",
       "      <td>46.94</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>148.461125</td>\n",
       "      <td>92.029770</td>\n",
       "      <td>LHS</td>\n",
       "      <td>0.298313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>46.94</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>152.642131</td>\n",
       "      <td>96.965960</td>\n",
       "      <td>LHS</td>\n",
       "      <td>0.317726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.400000</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>55.94</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>166.516162</td>\n",
       "      <td>91.170060</td>\n",
       "      <td>LHS</td>\n",
       "      <td>0.317726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.200000</td>\n",
       "      <td>2.362500</td>\n",
       "      <td>55.94</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>158.550633</td>\n",
       "      <td>84.867497</td>\n",
       "      <td>LHS</td>\n",
       "      <td>0.317726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.600000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>55.94</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>129.178102</td>\n",
       "      <td>79.334330</td>\n",
       "      <td>LHS</td>\n",
       "      <td>0.450332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.000607</td>\n",
       "      <td>0.400190</td>\n",
       "      <td>55.94</td>\n",
       "      <td>19.117603</td>\n",
       "      <td>0.258169</td>\n",
       "      <td>0.471908</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>143.813729</td>\n",
       "      <td>96.082226</td>\n",
       "      <td>NEHVI</td>\n",
       "      <td>0.462371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.94</td>\n",
       "      <td>15.898853</td>\n",
       "      <td>0.295208</td>\n",
       "      <td>0.543787</td>\n",
       "      <td>4.774095</td>\n",
       "      <td>134.427600</td>\n",
       "      <td>96.356114</td>\n",
       "      <td>NEHVI</td>\n",
       "      <td>0.482974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.222434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.94</td>\n",
       "      <td>14.783311</td>\n",
       "      <td>0.386174</td>\n",
       "      <td>0.530003</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>134.581296</td>\n",
       "      <td>96.511895</td>\n",
       "      <td>NEHVI</td>\n",
       "      <td>0.483332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.475599</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>55.94</td>\n",
       "      <td>17.073114</td>\n",
       "      <td>0.312364</td>\n",
       "      <td>0.620130</td>\n",
       "      <td>3.344044</td>\n",
       "      <td>134.000713</td>\n",
       "      <td>89.137855</td>\n",
       "      <td>NEHVI</td>\n",
       "      <td>0.483863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.650719</td>\n",
       "      <td>55.94</td>\n",
       "      <td>13.688209</td>\n",
       "      <td>0.298801</td>\n",
       "      <td>0.483599</td>\n",
       "      <td>3.975273</td>\n",
       "      <td>139.353450</td>\n",
       "      <td>97.818534</td>\n",
       "      <td>NEHVI</td>\n",
       "      <td>0.489422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Initial_temperature  Seed_load Seed_mean  Final_temperature  Cooling_rate  \\\n",
       "0            35.800000   1.837500     46.94          23.500000      0.220000   \n",
       "1            37.000000   1.312500     46.94          14.500000      0.460000   \n",
       "2            39.400000   0.787500     55.94          11.500000      0.140000   \n",
       "3            38.200000   2.362500     55.94          20.500000      0.380000   \n",
       "4            34.600000   0.262500     55.94          17.500000      0.300000   \n",
       "0            37.000607   0.400190     55.94          19.117603      0.258169   \n",
       "0            34.000000   0.000000     55.94          15.898853      0.295208   \n",
       "0            34.222434   0.000000     55.94          14.783311      0.386174   \n",
       "0            34.475599   0.014158     55.94          17.073114      0.312364   \n",
       "0            34.000000   0.650719     55.94          13.688209      0.298801   \n",
       "\n",
       "   AS_end_frac  AS_rate_mL_min         d90      Yield Method  Hypervolume  \n",
       "0     0.585000        9.300000  148.461125  92.029770    LHS     0.298313  \n",
       "1     0.495000        5.100000  152.642131  96.965960    LHS     0.317726  \n",
       "2     0.675000        7.900000  166.516162  91.170060    LHS     0.317726  \n",
       "3     0.855000        6.500000  158.550633  84.867497    LHS     0.317726  \n",
       "4     0.765000        3.700000  129.178102  79.334330    LHS     0.450332  \n",
       "0     0.471908        3.000000  143.813729  96.082226  NEHVI     0.462371  \n",
       "0     0.543787        4.774095  134.427600  96.356114  NEHVI     0.482974  \n",
       "0     0.530003        3.000000  134.581296  96.511895  NEHVI     0.483332  \n",
       "0     0.620130        3.344044  134.000713  89.137855  NEHVI     0.483863  \n",
       "0     0.483599        3.975273  139.353450  97.818534  NEHVI     0.489422  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z0_list[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecaeaf-4f0c-4e68-8768-501b2ec784f8",
   "metadata": {},
   "source": [
    "**Saving the data as csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c60a22-5b24-42b8-90e3-fd27a0af4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(Z0_list):\n",
    "    first_method = df['Method'].iloc[0]\n",
    "    last_method = df['Method'].iloc[-1]\n",
    "    exp = len(df) - 5\n",
    "\n",
    "    filename = f\"df_{exp}_{first_method}_{last_method}_campaign5.csv\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab6145-d02e-4c13-9a40-d99774b54615",
   "metadata": {},
   "source": [
    "**Figures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c87eb72-1c4a-4f07-af0d-c0d793adbb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial_temperature</th>\n",
       "      <th>Seed_load</th>\n",
       "      <th>Seed_mean</th>\n",
       "      <th>Final_temperature</th>\n",
       "      <th>Cooling_rate</th>\n",
       "      <th>AS_end_frac</th>\n",
       "      <th>AS_rate_mL_min</th>\n",
       "      <th>d90</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Method</th>\n",
       "      <th>Hypervolume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.093945</td>\n",
       "      <td>1.353894</td>\n",
       "      <td>55.94</td>\n",
       "      <td>21.418427</td>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.643533</td>\n",
       "      <td>8.411758</td>\n",
       "      <td>146.089986</td>\n",
       "      <td>91.022600</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.313644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.286633</td>\n",
       "      <td>0.684544</td>\n",
       "      <td>46.94</td>\n",
       "      <td>11.656175</td>\n",
       "      <td>0.322210</td>\n",
       "      <td>0.710433</td>\n",
       "      <td>5.964086</td>\n",
       "      <td>142.904591</td>\n",
       "      <td>87.735671</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.338949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.388577</td>\n",
       "      <td>2.526554</td>\n",
       "      <td>55.94</td>\n",
       "      <td>20.117607</td>\n",
       "      <td>0.434281</td>\n",
       "      <td>0.806839</td>\n",
       "      <td>7.692301</td>\n",
       "      <td>166.974901</td>\n",
       "      <td>88.743942</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.338949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.675751</td>\n",
       "      <td>0.499122</td>\n",
       "      <td>46.94</td>\n",
       "      <td>16.690746</td>\n",
       "      <td>0.202976</td>\n",
       "      <td>0.536472</td>\n",
       "      <td>3.057075</td>\n",
       "      <td>139.615032</td>\n",
       "      <td>96.337060</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.397480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.672997</td>\n",
       "      <td>2.060981</td>\n",
       "      <td>46.94</td>\n",
       "      <td>15.488294</td>\n",
       "      <td>0.281844</td>\n",
       "      <td>0.605451</td>\n",
       "      <td>6.531296</td>\n",
       "      <td>150.999505</td>\n",
       "      <td>93.209075</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.397480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38.889118</td>\n",
       "      <td>0.145706</td>\n",
       "      <td>55.94</td>\n",
       "      <td>17.688276</td>\n",
       "      <td>0.450722</td>\n",
       "      <td>0.735412</td>\n",
       "      <td>4.111902</td>\n",
       "      <td>142.422334</td>\n",
       "      <td>79.601737</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.397480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.342651</td>\n",
       "      <td>1.913070</td>\n",
       "      <td>46.94</td>\n",
       "      <td>13.037971</td>\n",
       "      <td>0.393264</td>\n",
       "      <td>0.895538</td>\n",
       "      <td>9.544317</td>\n",
       "      <td>163.151200</td>\n",
       "      <td>92.945435</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.397480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.653259</td>\n",
       "      <td>1.275127</td>\n",
       "      <td>55.94</td>\n",
       "      <td>23.902561</td>\n",
       "      <td>0.174459</td>\n",
       "      <td>0.462793</td>\n",
       "      <td>4.937263</td>\n",
       "      <td>146.379306</td>\n",
       "      <td>96.577549</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.398617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34.282478</td>\n",
       "      <td>2.336503</td>\n",
       "      <td>46.94</td>\n",
       "      <td>19.141594</td>\n",
       "      <td>0.346240</td>\n",
       "      <td>0.515423</td>\n",
       "      <td>6.949480</td>\n",
       "      <td>147.302522</td>\n",
       "      <td>96.620276</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.398814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38.092354</td>\n",
       "      <td>0.380288</td>\n",
       "      <td>55.94</td>\n",
       "      <td>13.901997</td>\n",
       "      <td>0.114850</td>\n",
       "      <td>0.842039</td>\n",
       "      <td>4.585600</td>\n",
       "      <td>145.599356</td>\n",
       "      <td>83.757033</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.398814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38.670658</td>\n",
       "      <td>1.493950</td>\n",
       "      <td>46.94</td>\n",
       "      <td>24.207062</td>\n",
       "      <td>0.228556</td>\n",
       "      <td>0.675321</td>\n",
       "      <td>9.945408</td>\n",
       "      <td>159.942464</td>\n",
       "      <td>89.420680</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.398814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36.387512</td>\n",
       "      <td>0.896938</td>\n",
       "      <td>55.94</td>\n",
       "      <td>12.632074</td>\n",
       "      <td>0.409873</td>\n",
       "      <td>0.664672</td>\n",
       "      <td>5.393867</td>\n",
       "      <td>148.083820</td>\n",
       "      <td>88.553094</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.398814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35.890892</td>\n",
       "      <td>1.796138</td>\n",
       "      <td>55.94</td>\n",
       "      <td>10.201728</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.483849</td>\n",
       "      <td>8.817307</td>\n",
       "      <td>154.840691</td>\n",
       "      <td>97.614090</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.402445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>39.677334</td>\n",
       "      <td>1.085697</td>\n",
       "      <td>46.94</td>\n",
       "      <td>23.003567</td>\n",
       "      <td>0.262110</td>\n",
       "      <td>0.860344</td>\n",
       "      <td>6.423494</td>\n",
       "      <td>159.686140</td>\n",
       "      <td>83.291060</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.402445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37.654140</td>\n",
       "      <td>2.268909</td>\n",
       "      <td>55.94</td>\n",
       "      <td>16.387402</td>\n",
       "      <td>0.193788</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>8.108108</td>\n",
       "      <td>159.558053</td>\n",
       "      <td>91.286076</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.402445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.347565</td>\n",
       "      <td>0.282577</td>\n",
       "      <td>46.94</td>\n",
       "      <td>20.524632</td>\n",
       "      <td>0.362605</td>\n",
       "      <td>0.584319</td>\n",
       "      <td>3.526741</td>\n",
       "      <td>136.552120</td>\n",
       "      <td>93.276928</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.428933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35.178776</td>\n",
       "      <td>2.146774</td>\n",
       "      <td>55.94</td>\n",
       "      <td>13.543275</td>\n",
       "      <td>0.223128</td>\n",
       "      <td>0.857164</td>\n",
       "      <td>5.762950</td>\n",
       "      <td>144.558428</td>\n",
       "      <td>86.120168</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.428933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37.395264</td>\n",
       "      <td>0.241908</td>\n",
       "      <td>46.94</td>\n",
       "      <td>23.294575</td>\n",
       "      <td>0.441957</td>\n",
       "      <td>0.501969</td>\n",
       "      <td>8.592594</td>\n",
       "      <td>145.028948</td>\n",
       "      <td>96.536278</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.428967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.837242</td>\n",
       "      <td>1.673463</td>\n",
       "      <td>55.94</td>\n",
       "      <td>14.807302</td>\n",
       "      <td>0.301736</td>\n",
       "      <td>0.567284</td>\n",
       "      <td>3.313947</td>\n",
       "      <td>167.512251</td>\n",
       "      <td>91.586760</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.428967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36.146748</td>\n",
       "      <td>1.045609</td>\n",
       "      <td>46.94</td>\n",
       "      <td>18.237745</td>\n",
       "      <td>0.132882</td>\n",
       "      <td>0.774383</td>\n",
       "      <td>7.456145</td>\n",
       "      <td>147.300731</td>\n",
       "      <td>90.210816</td>\n",
       "      <td>Sobol</td>\n",
       "      <td>0.428967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.524056</td>\n",
       "      <td>0.121283</td>\n",
       "      <td>46.94</td>\n",
       "      <td>19.391901</td>\n",
       "      <td>0.290838</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.639317</td>\n",
       "      <td>131.384754</td>\n",
       "      <td>82.905651</td>\n",
       "      <td>NParEGO</td>\n",
       "      <td>0.466846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.131449</td>\n",
       "      <td>55.94</td>\n",
       "      <td>20.284864</td>\n",
       "      <td>0.434473</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.964757</td>\n",
       "      <td>131.635798</td>\n",
       "      <td>84.828691</td>\n",
       "      <td>NParEGO</td>\n",
       "      <td>0.468047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.94</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.273331</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>6.382882</td>\n",
       "      <td>131.395426</td>\n",
       "      <td>84.487792</td>\n",
       "      <td>NParEGO</td>\n",
       "      <td>0.468096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.94</td>\n",
       "      <td>21.614560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.828337</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>126.075682</td>\n",
       "      <td>72.477398</td>\n",
       "      <td>NParEGO</td>\n",
       "      <td>0.499981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.445285</td>\n",
       "      <td>55.94</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.736196</td>\n",
       "      <td>128.815573</td>\n",
       "      <td>73.844219</td>\n",
       "      <td>NParEGO</td>\n",
       "      <td>0.500427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Initial_temperature  Seed_load Seed_mean  Final_temperature  Cooling_rate  \\\n",
       "0             35.093945   1.353894     55.94          21.418427      0.140832   \n",
       "1             37.286633   0.684544     46.94          11.656175      0.322210   \n",
       "2             39.388577   2.526554     55.94          20.117607      0.434281   \n",
       "3             35.675751   0.499122     46.94          16.690746      0.202976   \n",
       "4             36.672997   2.060981     46.94          15.488294      0.281844   \n",
       "5             38.889118   0.145706     55.94          17.688276      0.450722   \n",
       "6             38.342651   1.913070     46.94          13.037971      0.393264   \n",
       "7             34.653259   1.275127     55.94          23.902561      0.174459   \n",
       "8             34.282478   2.336503     46.94          19.141594      0.346240   \n",
       "9             38.092354   0.380288     55.94          13.901997      0.114850   \n",
       "10            38.670658   1.493950     46.94          24.207062      0.228556   \n",
       "11            36.387512   0.896938     55.94          12.632074      0.409873   \n",
       "12            35.890892   1.796138     55.94          10.201728      0.481000   \n",
       "13            39.677334   1.085697     46.94          23.003567      0.262110   \n",
       "14            37.654140   2.268909     55.94          16.387402      0.193788   \n",
       "15            35.347565   0.282577     46.94          20.524632      0.362605   \n",
       "16            35.178776   2.146774     55.94          13.543275      0.223128   \n",
       "17            37.395264   0.241908     46.94          23.294575      0.441957   \n",
       "18            39.837242   1.673463     55.94          14.807302      0.301736   \n",
       "19            36.146748   1.045609     46.94          18.237745      0.132882   \n",
       "0             34.524056   0.121283     46.94          19.391901      0.290838   \n",
       "0             34.000000   0.131449     55.94          20.284864      0.434473   \n",
       "0             34.000000   0.000000     46.94          25.000000      0.273331   \n",
       "0             34.000000   0.000000     55.94          21.614560      0.500000   \n",
       "0             34.000000   0.445285     55.94          25.000000      0.100000   \n",
       "\n",
       "    AS_end_frac  AS_rate_mL_min         d90      Yield   Method  Hypervolume  \n",
       "0      0.643533        8.411758  146.089986  91.022600    Sobol     0.313644  \n",
       "1      0.710433        5.964086  142.904591  87.735671    Sobol     0.338949  \n",
       "2      0.806839        7.692301  166.974901  88.743942    Sobol     0.338949  \n",
       "3      0.536472        3.057075  139.615032  96.337060    Sobol     0.397480  \n",
       "4      0.605451        6.531296  150.999505  93.209075    Sobol     0.397480  \n",
       "5      0.735412        4.111902  142.422334  79.601737    Sobol     0.397480  \n",
       "6      0.895538        9.544317  163.151200  92.945435    Sobol     0.397480  \n",
       "7      0.462793        4.937263  146.379306  96.577549    Sobol     0.398617  \n",
       "8      0.515423        6.949480  147.302522  96.620276    Sobol     0.398814  \n",
       "9      0.842039        4.585600  145.599356  83.757033    Sobol     0.398814  \n",
       "10     0.675321        9.945408  159.942464  89.420680    Sobol     0.398814  \n",
       "11     0.664672        5.393867  148.083820  88.553094    Sobol     0.398814  \n",
       "12     0.483849        8.817307  154.840691  97.614090    Sobol     0.402445  \n",
       "13     0.860344        6.423494  159.686140  83.291060    Sobol     0.402445  \n",
       "14     0.770530        8.108108  159.558053  91.286076    Sobol     0.402445  \n",
       "15     0.584319        3.526741  136.552120  93.276928    Sobol     0.428933  \n",
       "16     0.857164        5.762950  144.558428  86.120168    Sobol     0.428933  \n",
       "17     0.501969        8.592594  145.028948  96.536278    Sobol     0.428967  \n",
       "18     0.567284        3.313947  167.512251  91.586760    Sobol     0.428967  \n",
       "19     0.774383        7.456145  147.300731  90.210816    Sobol     0.428967  \n",
       "0      0.900000        5.639317  131.384754  82.905651  NParEGO     0.466846  \n",
       "0      0.900000        5.964757  131.635798  84.828691  NParEGO     0.468047  \n",
       "0      0.900000        6.382882  131.395426  84.487792  NParEGO     0.468096  \n",
       "0      0.828337        3.000000  126.075682  72.477398  NParEGO     0.499981  \n",
       "0      0.900000        3.736196  128.815573  73.844219  NParEGO     0.500427  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Z0_list[71]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0fe05d-93e3-457c-a589-16812d9735f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(df) + 1)\n",
    "y = df[[\"Hypervolume\"]].to_numpy().ravel()\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Hypervolume\", fontsize=12)\n",
    "plt.xlabel(\"Number of experiments\", fontsize=12)\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7e4d4-f706-427a-87a6-4d7137fcbc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
